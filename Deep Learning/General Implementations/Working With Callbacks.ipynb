{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQDRNrY2NCXf"
   },
   "source": [
    "<pre>\n",
    "1. Download the data from <a href='https://drive.google.com/file/d/15dCNcmKskcFVjs7R0ElQkR61Ex53uJpM/view?usp=sharing'>here</a>\n",
    "\n",
    "2. Code the model to classify data like below image\n",
    "\n",
    "<img src='https://i.imgur.com/33ptOFy.png'>\n",
    "\n",
    "3. Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.\n",
    "\n",
    "4. Save your model at every epoch if your validation accuracy is improved from previous epoch. \n",
    "\n",
    "5. you have to decay learning based on below conditions \n",
    "        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n",
    "               learning rate by 10%. \n",
    "        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n",
    "        \n",
    "6. If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n",
    "\n",
    "7. You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n",
    "\n",
    "8. Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\n",
    "\n",
    "9. use cross entropy as loss function\n",
    "\n",
    "10. Try the architecture params as given below. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w41Y3TFENCXk"
   },
   "source": [
    "<pre>\n",
    "<b>Model-1</b>\n",
    "<pre>\n",
    "1. Use tanh as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-2</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-3</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use he_uniform() as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-4</b>\n",
    "<pre>\n",
    "1. Try with any values to get better accuracy/f1 score.  \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.450564</td>\n",
       "      <td>1.074305</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.085632</td>\n",
       "      <td>0.967682</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117326</td>\n",
       "      <td>0.971521</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.982179</td>\n",
       "      <td>-0.380408</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.720352</td>\n",
       "      <td>0.955850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2  label\n",
       "0  0.450564  1.074305    0.0\n",
       "1  0.085632  0.967682    0.0\n",
       "2  0.117326  0.971521    1.0\n",
       "3  0.982179 -0.380408    0.0\n",
       "4 -0.720352  0.955850    0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('data.csv')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0.0\n",
      "1        0.0\n",
      "2        1.0\n",
      "3        0.0\n",
      "4        0.0\n",
      "        ... \n",
      "19995    0.0\n",
      "19996    1.0\n",
      "19997    1.0\n",
      "19998    0.0\n",
      "19999    0.0\n",
      "Name: label, Length: 20000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "Y = X['label']\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.450564</td>\n",
       "      <td>1.074305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.085632</td>\n",
       "      <td>0.967682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117326</td>\n",
       "      <td>0.971521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.982179</td>\n",
       "      <td>-0.380408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.720352</td>\n",
       "      <td>0.955850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2\n",
       "0  0.450564  1.074305\n",
       "1  0.085632  0.967682\n",
       "2  0.117326  0.971521\n",
       "3  0.982179 -0.380408\n",
       "4 -0.720352  0.955850"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = X.drop('label',axis = 1)\n",
    "X_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (20000, 2)\n",
      "Shape of Y: (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X:\",X_data.shape)\n",
    "print(\"Shape of Y:\",Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape : (14000, 2)\n",
      "X_test Shape : (6000, 2)\n",
      "Y_train Shape : (14000, 2)\n",
      "Y_test Shape : (6000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X_data,Y,test_size=0.30,stratify=Y)\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, 2) \n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, 2)\n",
    "print(\"X_train Shape :\",X_train.shape)\n",
    "print(\"X_test Shape :\",X_test.shape)\n",
    "print(\"Y_train Shape :\",Y_train.shape)\n",
    "print(\"Y_test Shape :\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all TensorFlow Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Input,Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import TerminateOnNaN\n",
    "import tensorflow.keras.backend as K\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to store the Loss, Auc and Micro-F1 score in variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_metrics(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.history={'Micro_F1': [],'AUC': [],'val_Micro_F1': [],'val_AUC': []}\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        ## on end of each epoch, we will get logs and update the self.history dict    \n",
    "        self.history['Micro_F1'].append(logs.get('Micro_F1'))\n",
    "        self.history['AUC'].append(logs.get('AUC'))\n",
    "        if logs.get('val_Micro_F1', -1) != -1:\n",
    "            self.history['val_Micro_F1'].append(logs.get('val_Micro_F1'))\n",
    "        if logs.get('val_AUC', -1) != -1:\n",
    "            self.history['val_AUC'].append(logs.get('val_AUC'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate micro-f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Micro_F1(y_true, y_pred):\n",
    "    true_pos_class_0 = K.sum(K.round(y_pred[:,0]) * y_true[:,0])\n",
    "    true_pos_class_1 = K.sum(K.round(y_pred[:,1]) * y_true[:,1])\n",
    "    false_pos_class_0 = K.sum(K.round(y_pred[:,0])) - true_pos_class_0\n",
    "    false_pos_class_1 = K.sum(K.round(y_pred[:,1])) - true_pos_class_1\n",
    "    false_neg_class_0 = K.sum(y_true[:,0]) - true_pos_class_0\n",
    "    false_neg_class_1 = K.sum(y_true[:,1]) - true_pos_class_1\n",
    "    sum_true_pos = true_pos_class_0 + true_pos_class_1\n",
    "    sum_false_pos = false_pos_class_0 + false_pos_class_1\n",
    "    sum_false_neg = false_neg_class_0 + false_neg_class_1\n",
    "    micro_precision = (sum_true_pos/(sum_true_pos + sum_false_pos))\n",
    "    micro_recall = (sum_true_pos/(sum_true_pos + sum_false_neg))\n",
    "    micro_F1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n",
    "    #tf.print(K.round(y_pred[:,1]) & y_true[:,1])\n",
    "    return micro_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to adjust the learning rate on the conditions mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeLearningRate(epoch,lr):\n",
    "    if(epoch > 1):\n",
    "        if(history_own.history['val_Micro_F1'][-1] < history_own.history['val_Micro_F1'][-2]):\n",
    "            lr = lr - ((10/100)*lr)\n",
    "    if(epoch%3 == 0):\n",
    "        lr = lr - ((5/100)*lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.009499999787658453.\n",
      "  2/875 [..............................] - ETA: 2:49 - loss: 0.7631 - AUC: 0.4082 - Micro_F1: 0.3750 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0363s). Check your callbacks.\n",
      "875/875 [==============================] - 3s 2ms/step - loss: 0.6979 - AUC: 0.5233 - Micro_F1: 0.5196 - val_loss: 0.6899 - val_AUC: 0.5525 - val_Micro_F1: 0.5377\n",
      "\n",
      "Epoch 00001: val_Micro_F1 improved from -inf to 0.53767, saving model to model_save\\weights-01-0.5525-0.5377.hdf5\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.009499999694526196.\n",
      "875/875 [==============================] - 1s 1ms/step - loss: 0.6936 - AUC: 0.5433 - Micro_F1: 0.5247 - val_loss: 0.6856 - val_AUC: 0.5525 - val_Micro_F1: 0.5377\n",
      "\n",
      "Epoch 00002: val_Micro_F1 did not improve from 0.53767\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.009499999694526196.\n",
      "875/875 [==============================] - 1s 1ms/step - loss: 0.6923 - AUC: 0.5433 - Micro_F1: 0.5243 - val_loss: 0.6908 - val_AUC: 0.5188 - val_Micro_F1: 0.5000\n",
      "\n",
      "Epoch 00003: val_Micro_F1 did not improve from 0.53767\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c4c93cf8b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input layer\n",
    "input_layer = Input(shape=(2,))\n",
    "#Dense hidden layer1\n",
    "layer1 = Dense(50,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(input_layer)\n",
    "#Dense hidden layer2\n",
    "layer2 = Dense(50,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(layer1)\n",
    "#Dense hidden layer3\n",
    "layer3 = Dense(50,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(layer2)\n",
    "#Dense hidden layer4\n",
    "layer4 = Dense(50,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(layer3)\n",
    "#Dense hidden layer5\n",
    "layer5 = Dense(50,activation='tanh',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(layer4)\n",
    "#output layer\n",
    "output = Dense(2,activation='softmax',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(layer5)\n",
    "#Creating a model\n",
    "model = Model(inputs=input_layer,outputs=output)\n",
    "\n",
    "#defining optimised\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01,momentum = 0.5)\n",
    "\n",
    "#Callbacks\n",
    "#store values of log\n",
    "history_own=loss_metrics() \n",
    "auc = tf.keras.metrics.AUC(name = 'AUC' )\n",
    "\n",
    "#ModelCheckpoint = Saves the model when the acc. metric improves\n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_AUC:.4f}-{val_Micro_F1:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_Micro_F1',  verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "#Lowers the learning rate systematically\n",
    "lrschedule = LearningRateScheduler(changeLearningRate, verbose=10)\n",
    "\n",
    "#Stops when the acc. metric does not imporve for 2 iterations\n",
    "earlystop = EarlyStopping(monitor='val_Micro_F1', patience=2, verbose=15,mode='max')\n",
    "\n",
    "#Creates tensorboard logs \n",
    "log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "\n",
    "#terminates when the loss becomes NaN\n",
    "TerminateWhenLossNaN = TerminateOnNaN()\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=[auc,Micro_F1])\n",
    "\n",
    "model.fit(X_train,Y_train,epochs=100, validation_data=(X_test,Y_test), batch_size=16,callbacks=[history_own,checkpoint,lrschedule,earlystop,TerminateWhenLossNaN,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.09500000141561031.\n",
      "  3/875 [..............................] - ETA: 8:48 - loss: 41016.8164 - AUC: 0.5243 - Micro_F1: 0.5128WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.2018s). Check your callbacks.\n",
      "875/875 [==============================] - 4s 3ms/step - loss: 141.3216 - AUC: 0.4972 - Micro_F1: 0.5000 - val_loss: 0.6952 - val_AUC: 0.5000 - val_Micro_F1: 0.5000\n",
      "\n",
      "Epoch 00001: val_Micro_F1 improved from -inf to 0.50000, saving model to model_save\\weights-01-0.5000-0.5000.hdf5\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0949999988079071.\n",
      "875/875 [==============================] - 1s 1ms/step - loss: 0.6944 - AUC: 0.5020 - Micro_F1: 0.4993 - val_loss: 0.6958 - val_AUC: 0.5000 - val_Micro_F1: 0.5000\n",
      "\n",
      "Epoch 00002: val_Micro_F1 did not improve from 0.50000\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0949999988079071.\n",
      "875/875 [==============================] - 1s 1ms/step - loss: 0.6947 - AUC: 0.5000 - Micro_F1: 0.5034 - val_loss: 0.6932 - val_AUC: 0.5000 - val_Micro_F1: 0.5000\n",
      "\n",
      "Epoch 00003: val_Micro_F1 did not improve from 0.50000\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c4cbf96d00>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input layer\n",
    "input_layer = Input(shape=(2,))\n",
    "#Dense hidden layer1\n",
    "layer1 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(input_layer)\n",
    "#Dense hidden layer2\n",
    "layer2 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(layer1)\n",
    "#Dense hidden layer3\n",
    "layer3 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(layer2)\n",
    "#Dense hidden layer4\n",
    "layer4 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(layer3)\n",
    "#Dense hidden layer5\n",
    "layer5 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(layer4)\n",
    "#output layer\n",
    "output = Dense(2,activation='softmax',kernel_initializer=tf.keras.initializers.RandomUniform(0, 1))(layer5)\n",
    "#Creating a model\n",
    "model = Model(inputs=input_layer,outputs=output)\n",
    "\n",
    "#defining optimised\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1,momentum = 0.5)\n",
    "\n",
    "#Callbacks\n",
    "#store values of log\n",
    "history_own=loss_metrics() \n",
    "auc = tf.keras.metrics.AUC(name = 'AUC' )\n",
    "\n",
    "#ModelCheckpoint = Saves the model when the acc. metric improve\n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_AUC:.4f}-{val_Micro_F1:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_Micro_F1',  verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "#Lowers the learning rate systematically\n",
    "lrschedule = LearningRateScheduler(changeLearningRate, verbose=10)\n",
    "\n",
    "#Stops when the acc. metric does not imporve for 2 iterations\n",
    "earlystop = EarlyStopping(monitor='val_Micro_F1', patience=2, verbose=15,mode='max')\n",
    "\n",
    "#Creates tensorboard logs \n",
    "log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "\n",
    "#terminates when the loss becomes NaN\n",
    "TerminateWhenLossNaN = TerminateOnNaN()\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=[auc,Micro_F1])\n",
    "\n",
    "model.fit(X_train,Y_train,epochs=100, validation_data=(X_test,Y_test), batch_size=16,callbacks=[history_own,checkpoint,lrschedule,earlystop,TerminateWhenLossNaN,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.009499999787658453.\n",
      "  3/875 [..............................] - ETA: 8:57 - loss: 0.7693 - AUC: 0.4510 - Micro_F1: 0.4583WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.2053s). Check your callbacks.\n",
      "875/875 [==============================] - 3s 3ms/step - loss: 0.6457 - AUC: 0.6781 - Micro_F1: 0.6246 - val_loss: 0.6230 - val_AUC: 0.7090 - val_Micro_F1: 0.6503\n",
      "\n",
      "Epoch 00001: val_Micro_F1 improved from -inf to 0.65033, saving model to model_save\\weights-01-0.7090-0.6503.hdf5\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.009499999694526196.\n",
      "875/875 [==============================] - 1s 1ms/step - loss: 0.6124 - AUC: 0.7231 - Micro_F1: 0.6644 - val_loss: 0.6087 - val_AUC: 0.7297 - val_Micro_F1: 0.6645\n",
      "\n",
      "Epoch 00002: val_Micro_F1 improved from 0.65033 to 0.66450, saving model to model_save\\weights-02-0.7297-0.6645.hdf5\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.009499999694526196.\n",
      "875/875 [==============================] - 1s 1ms/step - loss: 0.6082 - AUC: 0.7287 - Micro_F1: 0.6692 - val_loss: 0.6048 - val_AUC: 0.7327 - val_Micro_F1: 0.6672\n",
      "\n",
      "Epoch 00003: val_Micro_F1 improved from 0.66450 to 0.66717, saving model to model_save\\weights-03-0.7327-0.6672.hdf5\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.009024999709799886.\n",
      "875/875 [==============================] - 1s 1ms/step - loss: 0.6065 - AUC: 0.7309 - Micro_F1: 0.6679 - val_loss: 0.6005 - val_AUC: 0.7380 - val_Micro_F1: 0.6695\n",
      "\n",
      "Epoch 00004: val_Micro_F1 improved from 0.66717 to 0.66950, saving model to model_save\\weights-04-0.7380-0.6695.hdf5\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.009025000035762787.\n",
      "875/875 [==============================] - 1s 1ms/step - loss: 0.6064 - AUC: 0.7312 - Micro_F1: 0.6669 - val_loss: 0.5994 - val_AUC: 0.7393 - val_Micro_F1: 0.6735\n",
      "\n",
      "Epoch 00005: val_Micro_F1 improved from 0.66950 to 0.67350, saving model to model_save\\weights-05-0.7393-0.6735.hdf5\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.009025000035762787.\n",
      "875/875 [==============================] - 1s 1ms/step - loss: 0.6057 - AUC: 0.7316 - Micro_F1: 0.6681 - val_loss: 0.5993 - val_AUC: 0.7395 - val_Micro_F1: 0.6688\n",
      "\n",
      "Epoch 00006: val_Micro_F1 did not improve from 0.67350\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.007716375030577182.\n",
      "875/875 [==============================] - 1s 1ms/step - loss: 0.6040 - AUC: 0.7335 - Micro_F1: 0.6689 - val_loss: 0.6043 - val_AUC: 0.7333 - val_Micro_F1: 0.6692\n",
      "\n",
      "Epoch 00007: val_Micro_F1 did not improve from 0.67350\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c4cd604700>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input layer\n",
    "input_layer = Input(shape=(2,))\n",
    "#Dense hidden layer1\n",
    "layer1 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.HeUniform())(input_layer)\n",
    "#Dense hidden layer2\n",
    "layer2 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.HeUniform())(layer1)\n",
    "#Dense hidden layer3\n",
    "layer3 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.HeUniform())(layer2)\n",
    "#Dense hidden layer4\n",
    "layer4 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.HeUniform())(layer3)\n",
    "#Dense hidden layer5\n",
    "layer5 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.HeUniform())(layer4)\n",
    "#output layer\n",
    "output = Dense(2,activation='softmax',kernel_initializer=tf.keras.initializers.HeUniform())(layer5)\n",
    "#Creating a model\n",
    "model = Model(inputs=input_layer,outputs=output)\n",
    "\n",
    "#defining optimised\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01,momentum = 0.5)\n",
    "\n",
    "#Callbacks\n",
    "#store values of log\n",
    "history_own=loss_metrics() \n",
    "auc = tf.keras.metrics.AUC(name = 'AUC' )\n",
    "\n",
    "#ModelCheckpoint = Saves the model when the acc. metric improve\n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_AUC:.4f}-{val_Micro_F1:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_Micro_F1',  verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "#Lowers the learning rate systematically\n",
    "lrschedule = LearningRateScheduler(changeLearningRate, verbose=10)\n",
    "\n",
    "#Stops when the acc. metric does not imporve for 2 iterations\n",
    "earlystop = EarlyStopping(monitor='val_Micro_F1', patience=2, verbose=15,mode='max')\n",
    "\n",
    "#Creates tensorboard logs \n",
    "log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "\n",
    "#terminates when the loss becomes NaN\n",
    "TerminateWhenLossNaN = TerminateOnNaN()\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=[auc,Micro_F1])\n",
    "\n",
    "model.fit(X_train,Y_train,epochs=100, validation_data=(X_test,Y_test), batch_size=16,callbacks=[history_own,checkpoint,lrschedule,earlystop,TerminateWhenLossNaN,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
      "   3/1400 [..............................] - ETA: 15:35 - loss: 1.1228 - AUC: 0.4944 - Micro_F1: 0.5000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0008s vs `on_train_batch_end` time: 0.2234s). Check your callbacks.\n",
      "1400/1400 [==============================] - 5s 3ms/step - loss: 0.6472 - AUC: 0.6678 - Micro_F1: 0.6159 - val_loss: 0.6173 - val_AUC: 0.7159 - val_Micro_F1: 0.6488\n",
      "\n",
      "Epoch 00001: val_Micro_F1 improved from -inf to 0.64883, saving model to model_save\\weights-01-0.7159-0.6488.hdf5\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0009500000160187483.\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.6120 - AUC: 0.7232 - Micro_F1: 0.6614 - val_loss: 0.6019 - val_AUC: 0.7371 - val_Micro_F1: 0.6697\n",
      "\n",
      "Epoch 00002: val_Micro_F1 improved from 0.64883 to 0.66967, saving model to model_save\\weights-02-0.7371-0.6697.hdf5\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0009500000160187483.\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.6099 - AUC: 0.7265 - Micro_F1: 0.6624 - val_loss: 0.6027 - val_AUC: 0.7370 - val_Micro_F1: 0.6693\n",
      "\n",
      "Epoch 00003: val_Micro_F1 did not improve from 0.66967\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0008122500136960298.\n",
      "1400/1400 [==============================] - 2s 2ms/step - loss: 0.6056 - AUC: 0.7311 - Micro_F1: 0.6665 - val_loss: 0.6052 - val_AUC: 0.7329 - val_Micro_F1: 0.6680\n",
      "\n",
      "Epoch 00004: val_Micro_F1 did not improve from 0.66967\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c4cfb52ca0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input layer\n",
    "input_layer = Input(shape=(2,))\n",
    "#Dense hidden layer1\n",
    "layer1 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.HeUniform())(input_layer)\n",
    "#Dense hidden layer2\n",
    "layer2 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.HeUniform())(layer1)\n",
    "#Dense hidden layer3\n",
    "layer3 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.HeUniform())(layer2)\n",
    "#Dense hidden layer4\n",
    "layer4 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.HeUniform())(layer3)\n",
    "#Dense hidden layer5\n",
    "layer5 = Dense(50,activation='relu',kernel_initializer=tf.keras.initializers.HeUniform())(layer4)\n",
    "#output layer\n",
    "output = Dense(2,activation='softmax',kernel_initializer=tf.keras.initializers.HeUniform())(layer5)\n",
    "#Creating a model\n",
    "model = Model(inputs=input_layer,outputs=output)\n",
    "\n",
    "#defining optimised\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "#Callbacks\n",
    "#store values of log\n",
    "history_own=loss_metrics() \n",
    "auc = tf.keras.metrics.AUC(name = 'AUC' )\n",
    "\n",
    "#ModelCheckpoint = Saves the model when the acc. metric improve\n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_AUC:.4f}-{val_Micro_F1:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_Micro_F1',  verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "#Lowers the learning rate systematically\n",
    "lrschedule = LearningRateScheduler(changeLearningRate, verbose=10)\n",
    "\n",
    "#Stops when the acc. metric does not imporve for 2 iteration\n",
    "earlystop = EarlyStopping(monitor='val_Micro_F1', patience=2, verbose=15,mode='max')\n",
    "\n",
    "#Creates tensorboard logs \n",
    "log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "\n",
    "#terminates when the loss becomes NaN\n",
    "TerminateWhenLossNaN = TerminateOnNaN()\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=[auc,Micro_F1])\n",
    "\n",
    "model.fit(X_train,Y_train,epochs=100, validation_data=(X_test,Y_test), batch_size=10,callbacks=[history_own,checkpoint,lrschedule,earlystop,TerminateWhenLossNaN,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 7920), started 0:01:29 ago. (Use '!kill 7920' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b0b5ec1765cde684\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b0b5ec1765cde684\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Call_Backs_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
